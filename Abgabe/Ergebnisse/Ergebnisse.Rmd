---
title: "Ergebnisse"
author: "Franz Andersch & Niklas Münz"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    number_sections: true
    keep_tex: true
    includes:
      in_header: header.tex
    citation_package: biblatex
bibliography: Literatur.bib
---
\subsection{Vorgehensweise bei der Analyse}
Bevor die Ergebnisse erläutert werden, wird kurz auf die Vorgehensweise bei der Analyse eingegangen. In die Analyse werden fünf Modelle einbezogen: SVM mit linearem, polynomialem und radialem Kernel, sowie regularisierte logistische Regression und k-nearest neighbours.\newline
Vor dem erstellen der Modelle wird ein Tuning der Hyperparameter je Modell durchgeführt. Dafür wird die Bayesian Optimization Methode genutzt, welche ein iterativer Algorithmus ist. Hierbei werden die nächsten Evaluierungspunkte basierend auf zuvor beobachteten Ergebnissen bestimmt \parencite{yangHyperparameterOptimizationMachine2020}. Der Algorithmus basiert auf zwei Hauptkomponenten: einem Surrogatmodell und einer Akqusitionsfunktion. Das Surrogatmodell, wofür hier ein Gaussian Process genutzt wird, passt die bisher beobachteten Punkte an die Zielfunktion an. Die Akquisitionsfunktion wählt dann die nächsten Punkte aus, wobei ein Gleichgewicht zwischen der Erkundung neuer Bereiche und der Nutzung vielversprechender Regionen angestrebt wird. Dafür wird hier der Ansatz des Upper-Confidence-Bound genutzt, welcher obere Konfidenzgrenzen nutzt um den Verlust gegenüber der besten möglichen Entscheidung, während der Optimierung zu minimieren \parencite{snoekPracticalBayesianOptimization2012}.
\begin{align}
a_{UCB}(\mathbf{x};~\left\{\mathbf{x}_n,y_n\right\},\theta) = \mu(\mathbf{x};~\left\{\mathbf{x}_n,y_n\right\},\theta) + k \sigma(\mathbf{x};~\left\{\mathbf{x}_n,y_n\right\},\theta)
\end{align}
Die Bayesian Optimization wird genutzt, da sie eine schnelle Konvergenz für stetige Hyperparameter aufweist \parencite{yangHyperparameterOptimizationMachine2020}. Als Evaluierungskriterium wird die Genauigkeit der Modelle, welche durch den Anteil der korrekt klassifizierten Beobachtungen wiedergegeben wird, verwendet. Zur Bestimmung dieser werden die erzeugten Testdaten herangezogen.

Basierend auf den Ergebnissen des Tuning werden die oben genannten Modelle erstellt. Daraufhin werden die Genauigkeit, die Receiver Operating Characteristic Kurve (ROC-Kurve) bzw. der Area Under The Curve Wert (AUC-Wert) und der F1-Score für jedes Modell bestimmt.\newline
Die ROC-Kurve ist eine grafische Darstellung der Leistungsfähigkeit eines Klassifikationsmodells, wobei die Sensitivität auf der y-Achse von 0 bis 1 gegen die Spezifität auf der x-Achse von 1 bis 0 abgetragen wird \parencite{fawcettIntroductionROCAnalysis2006}. Sensitivität und Spezifität ergeben sich aus:
\begin{align}
Sensitivität=\frac{korrekt~Positiv}{korrekt~Positiv~+~falsch~Negativ}
\end{align}
\begin{align}
Spezifität=\frac{korrekt~Negativ}{falsch~Positiv~+~korrekt~Negativ}
\end{align}
Positiv ist in diesem Fall gleichbedeutend mit Klasse 1 und Negativ mit Klasse 2. Die ROC-Kurve zeigt dann den Zusammenhang zwischen dem Nutzen (korrekt Positive) und den Kosten (falsch Positive) auf. Eine ideale Kurve läuft nah am linken, oberen Rand der Grafik, da hier bereits bei sehr hoher Spezifität (hohe Anzahl korrekt Negative) eine hohe Sensitivität (hohe Anzahl korrekt Positive) erreicht wird.  Der AUC-Wert bezieht sich auf die Fläche unterhalb der Kurve und liegt somit im Intervall [0,1], wobei ein Wert von 1 für eine perfekte Klassifikation spricht, während ein Wert von 0.5 gleichbedeutend mit einer rein zufälligen Zuordnung der Klassen spricht.\newline
Für den F1-Score ist außerdem die Präzision von Bedeutung, die sich wie folgt berechnet \parencite{fawcettIntroductionROCAnalysis2006}:
\begin{align}
Präzision=\frac{korrekt~Positiv}{korrekt~Positiv~+~falsch~Positiv}
\end{align}
Der F1-Score beschreibt das harmonische Mittel zwischen Präzision und Sensitivität und drückt folglich die Fähigkeit des Modells, gleichzeitig falsch Positive und falsch Negative zu minimieren, aus.
\begin{align}
F1\text{-}Score=\frac{2}{1/Präzision~+~1/Sensitivität}
\end{align}\newline
Zuletzt wird innerhalb jeder Datensituation, für jeden Algorithmus ein Rang vergeben, der sich aus der Summe der drei Auswertungskriterien Genauigkeit, AUC-Wert sowie F1-Score ergibt. Dabei steht Rang 1 für den am besten abschneidenden Algorithmus in der jeweiligen Datensituation.

\subsection{Darstellung der Ergebnisse}
Tabelle \ref{tab:TabVergleich} zeigt die Leistungsfähigkeit der fünf Klassifikationsalgorithmen über die neun verschiedenen Datensituationen anhand drei verschiedener Evaluationskriterien, sowie den Rang in der jeweiligen Datensituation.\newline
Betrachtet man den durchschnittlichen Rang über alle Szenarien hinweg, dann kann \textbf{H1} zumindest in Teilen bestätigt werden. Mit einem durchschnittl. Rang von 2.33 und 2.55 belegen jeweils \textit{SVM-P} und \textit{SVM-R} die vordersten Plätze. Allerdings muss auch gesagt, dass \textit{SVM-L} nach den beiden anderen Klassifikationsmethoden den letzten Platz belegt.
\afterpage{
\clearpage
\begin{landscape}

```{r TabVergleich, echo=FALSE, results='asis', warning=FALSE,fig.align='center'}
library(gridExtra)
library(knitr)
library(xtable)

load(file = "Tabellen/Tabellen_S1_S4_S7.RData")
load(file = "Tabellen/Tabellen_S2_S5_S8.RData")
load(file = "Tabellen/Tabellen_S3_S6_S9.RData")

cat(
  "\\begin{table}[h]",
  "\\centering",
  "\\begin{tabular}{|c|c|c|c|}",
  "\\hline\n",
  "& \\textbf{linear} & \\textbf{polynomial} & \\textbf{radial} \\\\",
  "\\hline",
  # p << n
  "\\multirow{5}{*}{\\textbf{$p \\ll n$}} & ",
  kable(S1_Tabelle, format = "latex", booktabs = TRUE, linesep = "", row.names = TRUE), " & ",
  kable(S2_Tabelle, format = "latex", booktabs = TRUE, linesep = "", row.names = TRUE), " & ",
  kable(S3_Tabelle, format = "latex", booktabs = TRUE, linesep = "", row.names = TRUE), " \\\\",
  "\\hline",
  # p = n
  "\\multirow{5}{*}{\\textbf{$p \\approx n$}} & ",
  kable(S4_Tabelle, format = "latex", booktabs = TRUE, linesep = "", row.names = TRUE), " & ",
  kable(S5_Tabelle, format = "latex", booktabs = TRUE, linesep = "", row.names = TRUE), " & ",
  kable(S6_Tabelle, format = "latex", booktabs = TRUE, linesep = "", row.names = TRUE), " \\\\",
  "\\hline",
  # p >> n
  "\\multirow{5}{*}{\\textbf{$p \\gg n$}} & ",
  kable(S7_Tabelle, format = "latex", booktabs = TRUE, linesep = "", row.names = TRUE), " & ",
  kable(S8_Tabelle, format = "latex", booktabs = TRUE, linesep = "", row.names = TRUE), " & ",
  kable(S9_Tabelle, format = "latex", booktabs = TRUE, linesep = "", row.names = TRUE), " \\\\",
  "\\hline",
  "\\end{tabular}",
  "\\caption{Vergleich der Modelle}",
  "\\label{tab:TabVergleich}",
  "\\end{table}"
)

```

\end{landscape}
\clearpage
}
\newline In den Datensituationen mit linearer Form der Entscheidungsgrenze performt insbesondere \textit{logR} gut, da sie in allen drei Szenarien über alle Kriterien hinweg einen Wert von mindestens 0.72 aufweist und jeweils mindestens Rang 2 belegt. Im Falle von $p \ll n$ und $p \approx n$ zeigen auch \textit{SVM-L}, \textit{SVM-P} und \textit{SVM-R} eine gute Leistung mit Werten nahe 0.9 bzw. 0.8. In dem hochdimensionalen Setting $p \gg n$ schneiden alle Drei, jedoch insbesondere \textit{SVM-R} schlechter ab, welche einen AUC-Wert von 0.5 aufweist. \textit{K-NN} belegt im linearen Kontext für $p \ll n$ und $p \approx n$ jeweils den fünften Rang, wobei der Unterschied im niedrigdimensionalen Raum am deutlichsten ist (0.39 schlechtere Genauigkeit als der nächst schlechteste Algorithmus). Im hochdimensionalen Raum kann \textit{K-NN} allerdings als bester Algorithmus mit Werten über 0.8 überzeugen.\newline
Die ROC-Kurven werden nur in den Datensituationen mit $p \ll n$ genutzt, da die grafische Darstellung in Szenarien mit kleinem $n$ aufgrund der geringen Anzahl der Datenpunkte nicht sinnvoll erscheint. Des Weiteren ist es wichtig zu erwähnen, das für die Berechnung der ROC-Kurven die Wahrscheinlichkeit, dass eine Beobachtung zu einer bestimmten Klasse gehört, benötigt wird. Da SVM anders als beispielsweise logistische Regression, nicht direkt eine Wahrscheinlichkeit ausgeben, gibt es aber auch hierfür Möglichkeiten diese zu berechnen (siehe \cite{plattProbabilisticOutputsSupport2000}).\newline
Abbildung \ref{fig:S1ROC} zeigt die ROC-Kurven für die Datensituation S1. Diese zeigt grafisch noch einmal den deutlichen Unterschied zwischen \textit{K-NN}, welcher nur etwas besser als eine reine Zufallsauswahl ist und den anderen vier Algorithmen, welche nahezu perfekt klassifizieren. Im Bezug auf \textbf{H2} sieht die Beweislage also eher dürftig aus, da \textit{SVM-L} durschschnittlich in den linearen Szenarien eher im Mittelfeld rangiert.

```{r S1ROC, echo=FALSE, warning=FALSE, message=FALSE, fig.show='hide', fig.cap="ROC-Kurven Szenario 1",fig.align='center'}
library(pROC)
load(file = "ROC/S1_roc.RData")
plot(S1_roc_linear, col = "blue")
plot(S1_roc_polynomial, col = "red", add = TRUE)
plot(S1_roc_radial, col = "green", add = TRUE)
plot(S1_roc_logR, col = "violet", add = TRUE)
plot(S1_roc_k_NN, col = "orange", add = TRUE)
legend("bottomright",
       legend = c(paste("linear (AUC:", round(auc(S1_roc_linear), 3), ")"), paste("polynomial (AUC:", round(auc(S1_roc_polynomial), 3), ")"), paste("radial (AUC:", round(auc(S1_roc_radial),3 ), ")"), paste("logR (AUC:", round(auc(S1_roc_logR), 3), ")"), paste("k_NN (AUC:", round(auc(S1_roc_k_NN), 3), ")")),
       col = c("blue", "red", "green", "violet", "orange"),
       lwd = 2)
```

In den Datensituationen mit polynomialer Form der Entscheidungsgrenze ist die Leistungsfähigkeit aller Algorithmen grundsätzlich schlechter als in den zuvor beschriebenen Szenarien. Im Fall von $p \ll n$ sind die Ergebnisse ausschließlich für \textit{SVM-R} sehr gut mit Werten über 0.9. Die restlichen Algorithmen performen mittelmäßig mit Werten zwischen 0.7 und 0.8, wobei sich hier kein Algorithmus von den anderen absetzen kann. Bei $p \approx n$ ist \textit{K-NN} den anderen Klassifikationsmethoden leicht überlegen, insbesondere bei einem AUC-Wert von 0.729. Die anderen Algorithmen befinden sich durchweg über alle Werte hinweg nahe 0.5. Ähnlich ist das der Fall für $p \gg n$ bei dem auch wieder \textit{K-NN} eine leicht bessere Leistung zeigt mit einer Genauigkeit von 0.7, gefolgt von \textit{LogR} mit 0.64. Dennoch ist für die letzten beiden Szenarien deutlich, dass kein Algorithmus gute Leistung zeigt.\newline
Abbildung \ref{fig:S2ROC} zeigt die ROC-Kurven für Szenario 2. Aus dieser Abbildung ist ersichtlich, dass sich \textit{SVM-R} deutlich von den anderen Algorithmen abheben kann, welche alle eine mittelmäßige Leistung zeigen. Wir können also hier keine Bestätigung für \textbf{H2} finden, nach der wir eigentlich erwarten würden, dass \textit{SVM-P} hier besser Klassifiziert. 

```{r S2ROC, echo=FALSE, warning=FALSE, message=FALSE,fig.show='hide',fig.cap="ROC-Kurven Szenario 2",fig.align='center'}
library(pROC)

load(file = "ROC/S2_roc.RData")

plot(S2_roc_linear, col = "blue")
plot(S2_roc_polynomial, col = "red", add = TRUE)
plot(S2_roc_radial, col = "green", add = TRUE)
plot(S2_roc_logR, col = "violet", add = TRUE)
plot(S2_roc_k_NN, col = "orange", add = TRUE)
legend("bottomright",
       legend = c(paste("linear (AUC:", round(auc(S2_roc_linear), 3), ")"), paste("polynomial (AUC:", round(auc(S2_roc_polynomial), 3), ")"), paste("radial (AUC:", round(auc(S2_roc_radial), 3), ")"), paste("logR (AUC:", round(auc(S2_roc_logR), 3), ")"), paste("k_NN (AUC:", round(auc(S2_roc_k_NN), 3), ")")),
       col = c("blue", "red", "green", "violet", "orange"),
       lwd = 2)
```

In den Datensituationen mit radialer Form der Entscheidungsgrenze ist für den Fall $p \ll n$ eine eindeutige Unterscheidung zu treffen. Während \textit{SVM-P}, \textit{SVM-R} und \textit{K-NN} gut klassifizieren, haben dabei \textit{SVM-L} und \textit{LogR} große Probleme und zeigen lediglich Werte nahe 0.5. Insbesondere \textit{SVM-P} zeigt mit einem AUC-Wert von 0.981 eine sehr gute Performance. Weniger drastisch aber dennoch mit dem gleichen Resultat ist dies für $p \approx n$ der Fall. Während \textit{SVM-P} im Vergleich zum vorherigen Szenario etwas schlechter performt, bleibt \textit{SVM-R} nahezu identisch und \textit{K-NN} kann sich sogar leicht steigern auf einen AUC-Wert von 0.8. \textit{SVM-L} und \textit{LogR} können sich leicht verbessern (beispielsweise mit einer Genauigkeit nahe 0.6), belegen dennoch weiterhin Rang vier und fünf. Für $p \gg n$ sind ebenfalls die beiden Gruppen zu differenzieren. Für \textit{SVM-L} und \textit{LogR} sind wie in Szenario 3 wieder Werte nahe 0.5 für die Genauigkeit erkennbar. \textit{SVM-P} und \textit{K-NN} performen in diesem Szenario am Besten. \textit{SVM-R} fällt insbesondere mit einer Genauigkeit von 0.66 im Vergleich zu 0.76 etwas hinter den anderen beiden Algorithmen zurück.\newline
Abbildung \ref{fig:S3ROC} zeigt die ROC-Kurven für Szenario 3. Hieraus wird deutlich, dass die Kurve von \textit{SVM-P} nahe an der oberen, linken Ecke verläuft, was für eine nahezu perfekte Klassifikation spricht. Der etwas flachere Verlauf von \textit{SVM-R} und \textit{K-NN} macht deutlich, dass diese beiden Algorithmen weniger gute Leistung zeigen. Dennoch können die beiden oben erwähnten Gruppen gut identifiziert werden, da sich \textit{SVM-L} und \textit{LogR} nahe der diagonalen Linie orientieren. Diese beschreibt eine ROC-Kurve einer reinen Zufallsauswahl, woraus gefolgert werden kann, dass sie kaum Fähigkeit aufweisen zwischen den Klassen zu unterscheiden. Diese Szenarien bestätigen die Vermutung aus \textbf{H2} zwar nicht direkt, da hier im Ranking über alle Szenarien mit radialer Entscheidungsgrenze hinweg interessanterweise \textit{SVM-P} am besten performt aber zumindest liegt hier \textit{SVM-R} auf dem zweiten Platz. Trotzdem müssen wir mit diesen Ergebnissen \textbf{H2} ablehnen.

```{r S3ROC, echo=FALSE, warning=FALSE, message=FALSE, fig.width=7, fig.height=5, fig.cap="ROC-Kurven Szenario 3",fig.align='center',fig.show='hide'}
library(pROC)

load(file = "ROC/S3_roc.RData")

plot(S3_roc_linear, col = "blue")
plot(S3_roc_polynomial, col = "red", add = TRUE)
plot(S3_roc_radial, col = "green", add = TRUE)
plot(S3_roc_logR, col = "violet", add = TRUE)
plot(S3_roc_k_NN, col = "orange", add = TRUE)
legend("bottomright",
       legend = c(paste("linear (AUC:", round(auc(S3_roc_linear), 3), ")"), paste("polynomial (AUC:", round(auc(S3_roc_polynomial), 3), ")"), paste("radial (AUC:", round(auc(S3_roc_radial), 3), ")"), paste("logR (AUC:", round(auc(S3_roc_logR), 3), ")"), paste("k_NN (AUC:", round(auc(S3_roc_k_NN), 3), ")")),
       col = c("blue", "red", "green", "violet", "orange"),
       lwd = 2)
```

\begin{figure}[htb]
\begin{minipage}{0.48\linewidth}
\centering
\includegraphics{Ergebnisse_files/figure-latex/S1ROC-1.pdf}
\caption{ROC-Kurven Szenario 1}
\label{fig:S1ROC}
\end{minipage}
\hfill
\begin{minipage}{0.48\linewidth}
\centering
\includegraphics{Ergebnisse_files/figure-latex/S2ROC-1.pdf}
\caption{ROC-Kurven Szenario 2}
\label{fig:S2ROC}
\end{minipage}
\vspace*{0.5cm}\newline
\begin{minipage}{\linewidth}
\centering
\includegraphics[width=0.48\linewidth]{Ergebnisse_files/figure-latex/S3ROC-1.pdf}
\caption{ROC-Kurven Szenario 3}
\label{fig:S3ROC}
\end{minipage}
\end{figure}

Zuletzt wird ein Vergleich der einzelnen Dimensionierungen über die drei Formen der Entscheidungsgrenze gezogen. So ist für $p \ll n$ ersichtlich, dass insbesondere \textit{SVM-P} und \textit{SVM-R} über alle drei Szenarien gute Leistung (mit Werten mindestens nahe 0.8). Auch im durchschnittlichen Ranking für diese Szenarien  über alle drei Szenarien zeigen die \textit{SVM}-Methoden die besten Ergebnisse. Überaschend ist nur die unerwartet schlechte Performance von \textit{K-NN}. Somit bestätigt sich \textbf{H3} hier nur teilweise. In den hochdimensionalen Szenarien $p \gg n$ ist einzig \textit{K-NN} überzeugend, welcher auch im polynomialen Szenario Werte über 0.7 aufweist. Hier belegen die \textit{SVM}-Methoden tatsächlich sogar die letzten Plätze im durschschnittlichen Ranking. Dies deutet klar darauf hin, dass \textbf{H4} mit unseren Daten verworfen werden muss.
