---
title: "Einleitung"
author: "Franz Andersch & Niklas Münz"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    number_sections: true
    keep_tex: true
    includes:
      in_header: header.tex
    citation_package: biblatex
bibliography: Literatur.bib
---
Bei der Separierung von zwei Datenklassen scheint eine lineare Trennlinie zwischen die Klassen zu ziehen eine simple und intuitive Methode zu sein. Genau auf dieser Idee basieren \textit{Support Vector Machines} (\textit{SVM}). Jedoch sind Daten nicht immer perfekt trennbar oder es liegt keine lineare Trennlinie vor. Wie \textit{SVM} funktionieren, wie sie mit solchen Situationen umgehen, wie sie unter vorgegebenen Bedingungen performen und welche Schlussfolgerungen für die praktische Anwendung gezogen werden können, soll in dieser Arbeit beleuchtet werden.

Die Grundlage für eine optimal separierende Hyperebene wurde bereits 1964 von Alexej Chervonenkis und Vladimir Vapnik gelegt und die Methode wurde anschließend von mehreren Autoren stetig erweitert \parencite{vapnikEstimationDependencesBased2006}. Seitdem ist ihre Leistung, trotz großer Fortschritte im Bereich neuronaler Netze, vor allem was binärer Klassifikationsprobleme angeht unumstritten und sie gelten dabei als eine der meistgenutzten Methoden \parencite{soofiClassificationTechniquesMachine2017}. Dabei bieten \textit{SVM} eine große Anzahl an Anwendungsmöglichkeiten, wie beispielsweise, in der Bildklassifikation, in der Bioinformatik (Krebsklassifikation) oder im Aufdecken von Kreditkartenbetrug \parencite{cervantesComprehensiveSurveySupport2020}. 

In dieser Arbeit wollen wir zuerst die Funktionsweise der \textit{SVM} näher betrachten. Dazu wird zunächst gezeigt wie die Konstruktion dieser separierenden Hypereben im Falle linear trennbarer Daten funktioniert. Daraufhin wird die Idee eines Soft Margin Classifier aufgegriffen, der sich dem Problem nicht trennbarer Daten annimmt. Zuletzt geht es um die Anwendung von Kernels, die es ermöglichen, nicht lineare Entscheidungsgrenzen herzustellen.\newline
In Kapitel 3 werden dann die Vor- und Nachteile der Methode betrachtet. Anhand dieser Informationen werden Schlussfolgerungen zur Leistung der \textit{SVM} gezogen. Darauf aufbauend wird im folgenden Kapitel der Aufbau unseres Experiments beschrieben. Wir generieren uns zum anschließenden Vergleich neun verschiedene Datenszenarien, welche in ihrer Dimensionalität und Komplexität der Entscheidungsgrenze variieren. Für die Evaluierung der Klassifikationsleistung werden neben den \textit{SVM} weitere Klassifikationsmethoden herangezogen.\newline
Im Kapitel Hypothesen wurden auf Basis von Literatur, in der bereits ähnliche Versuche durchgeführt wurden, Vermutungen aufgestellt, wie die einzelnen Klassifikationsmethoden im Vergleich abschneiden werden.\newline
Es folgen die Ergebnisse, in denen die Durchführung des Experiments beschrieben und die Leistungen anhand verschiedener Maßzahlen evaluiert werden. Darüber hinaus wird überprüft, ob sich die Hypothesen, die wir zuvor aufgestellt haben bewahrheitet haben. Im letzten Abschnitt wird ein Fazit gezogen, indem die Ergebnisse diskutiert und mögliche Ansätze zur Verbesserung unserer Arbeit vorgeschlagen werden.

