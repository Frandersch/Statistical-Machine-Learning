---
title: "Einleitung"
author: "Franz Andersch & Niklas Münz"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    number_sections: true
    keep_tex: true
    includes:
      in_header: header.tex
    citation_package: biblatex
bibliography: Literatur.bib
---
Bei der Separierung von zwei Datenklassen scheint eine lineare Trennlinie zwischen die Klassen zu ziehen eine simple und intuitive Methode zu sein. Genau auf dieser Idee basieren Support Vector Machines (SVM). Jedoch sind Daten nicht immer perfekt trennbar oder es liegt keine lineare Trennlinie vor. Wie SVM funktionieren, wie sie mit solchen Situationen umgehen, wie sie unter vorgegebenen Bedingungen performen und welche Schlussfolgerungen für die praktische Anwendung gezogen werden können, soll in dieser Arbeit beleuchtet werden.

Die Grundlage für eine optimal separierende Hyperebene wurde bereits 1964 von Alexej Chervonenkis und Vladimir Vapnik gelegt und die Methode wurde anschließend von mehreren Autoren stetig erweitert \parencite{vapnikEstimationDependencesBased2006}. Seitdem ist ihre Leistung, trotz großer Fortschritte im Bereich neuronaler Netze, vor allem im Bereich binärer Klassifikationprobleme unumstritten und sie gelten dabei als eine der meistgenutzten Methoden \parencite{soofiClassificationTechniquesMachine2017}. Dabei bieten SVM eine große Anzahl an Anwendungsmöglichkeiten, wie beipielsweise, in der Bildklassifikation, in der Bioinformatik (Krebsklassifikation) oder im Aufedecken von Kreditkartenbetrug \parencite{cervantesComprehensiveSurveySupport2020}. 

In dieser Arbeit wollen wir zuerst die Funktionsweise der SVM näher betrachten. Dazu wird zunächst gezeigt wie die Konstruktion dieser separierenden Hypereben im Falle linear trennbarer Daten funktioniert. Daraufhin wird die Idee eines Soft Margin Klassifier aufgegriffen, der sich dem Problem nicht trennbarer Daten annimmt. Zuletzt geht es um die Anwendung von Kernels, die es ermöglichen, nichtlineare Entscheidungsgrenzen herzustellen.\newline
In Kapitel 3 werden dann die Vor und Nachteile der Methode betrachtet. Anhand dieser Informationen werden Schlussfolgerungen zur Leistung der SVM gezogen. Darauf aufbauend wird im folgenden Kapitel der Aufbau unseres Experiments beschrieben. Wir generieren uns zum anschließenden Vergleich neun verschiedene Datenszenarien, welche in ihrer Dimensionalität und Kompleixität der Entscheidungsgrenze variieren. Für die Evaluierung der Klassifikationsleistung werden neben den SVM weitere Klassifikationsmtehoden herangezogen.\newline
Im Kapitel Hypothesen wurden auf Basis von Literatur, in der bereits ähnliche Versuche durchgeführt wurden, Vermutungen aufgestellt, wie die einzelnen Klassifikationsmethoden im Vergleich abschneiden werden.\newline
Es folgen die Ergebnisse, in denen die Durchführung des Experiments beschrieben und die Leistungen anhand verschiedener Maßzahlen evaluiert werden. Darüber hinaus wird überprüft, ob sich die Hypothesen die wir zuvor aufgestellt haben bewahrheitet haben. Im letzten Abschnitt wird ein Fazit gezogen, indem die Ergebnisse noch einmal diskutiert und Defizite in unserer Arbeit aufgearbeitet werden.

