% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage[german]{babel}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{csquotes}
\AtBeginDocument{
\renewcommand{\maketitle}{}
}
\PassOptionsToPackage{a4paper,margin = 2.5cm}{geometry}
\usepackage{geometry}
\usepackage{float}
\newcommand{\bcenter}{\begin{center}}
\newcommand{\ecenter}{\end{center}}
\renewcommand{\contentsname}{Inhalt}
\usepackage{blindtext}
\usepackage[backend=biber, style = apa]{biblatex}
\addbibresource{Literatur.bib}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{biblatex}
\addbibresource{Literatur.bib}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Zielsetzung und Datengenerierung},
  pdfauthor={Franz Andersch \& Niklas Münz},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Zielsetzung und Datengenerierung}
\author{Franz Andersch \& Niklas Münz}
\date{2024-08-23}

\begin{document}
\maketitle

Das Ziel dieser Arbeit ist es, in verschiedenen Datensituationen die
performance von SVM Algorithmen für die binäre Klassifikation zu
evaluieren. Dafür wollen wir eine Reihe von Datensätzen mit
verschiedenen Charakteristiken synthetisch erstellen und entsprechen als
Trainigsdaten verwenden. Die Datensätze unterscheiden sich in zwei
zentralen Eigenschaften. Das erste sind die Dimensionen. Es soll
unterschieden werden in drei Kategorien. Die erste ist, dass es deutlich
mehr Beobachtungen als Variablen gibt, also \(n \gg p\). Ein solches
Datenszenario kann z.B. im Kontext des Zensus auftreten, in dem eine
große Anzahl an Personen befragt wird, aber die Vorgabe besteht, dass
die Bürger nicht zu stark belastet werden sollen, weshalb nur einige
wenige Kernfragen gestellt werden. In diesem Fall wurde für \(n=1000\)
Beobachtungen und \(p=10\) Variablen generiert. Das zweite Szenario
stellt Datensätze vor die etwa gleich viele Beobachtung wie Variablen
haben \(n \approx p\). Ein solches Szenario kann in vielen Kontexten
auftreten. Für dieses Szenario sind die Dimensionen \(n=p=50\). Das
letzte Szenario behandelt dann entsprechend den Fall \(n \ll p\). Dies
tritt oft im Kontext von Datenerhebungen im medizinischen Bereich auf,
da sehr viele Erhebungen zu kostspielig wären. Auch im Bereich des
Natural Language Processing sind solche Datensätze häufiger anzutreffen
\parencite{scholzComparisonClassificationMethods2021}. Die Werte die
dafür angenommen wurden sind \(p=200\) und \(n=50\).

Die zweite Charakteristik ist der Datengenerierende Prozess. Da in
dieser Arbeit SVMs im Vordergrund stehen und wir hier vor allem Zeigen
wollen, wie SVMs funktionieren, wird der DGP so aufgebaut, dass er der
grundlegenden Idee der SVMs am ehesten Entspricht. Die Vorgehensweise
ist, im ersten Schritt eine Hyperplane, im \(p\)-Dimensionalen Raum, in
einer bestimmte Form zu erstellen und anschließend auf jeweils einer
Seite dieser Hyperplane \(n/2\) zufällige Punkte zu samplen, die die
jeweilige Ausprägung in der Zielvariable repräsentieren.

Insgesammt gibt es auch hier wieder 3 Kategorien. Die erste sind linear
getrennte Daten. Dafür wird eine lineare Hyperplane der Form
\begin{align*}
\beta_0+\beta_1 X_1+\beta_2 X_2 +...+\beta_{1-p} = X_p
\end{align*} mit zufälligen Koeffizienten erzeugt. Diese Daten sollen
also den Annahmen entsprechen, die für SVMs mit linearem Kernel gelten.
Nachem für eine Beobachtung \(j\) zufällig ein Punkt auf der Ebene
gesampelt wurde, wurde dieser andschließend verschoben. Die Verschiebung
erfolgte über eine Skalierung des normierten Normalenvektors
\(k\left(\frac{\overline{\beta}}{||\overline{\beta}||}\right)\). \(k\)
ist dabei eine normalverteilte Zufallszahl mit Mittelwert \(\mu_k\) und
Varianz \(\sigma^2_k\). Dieser Prozess wird \(n/2\) mal wiederholt für
die eine Ausprägung der Zielvariable und dementsprechen \(n/2\) mal für
die andere Ausprägung, dann aber mit \(-\mu_k\) als Mittelwert für
\(k\). \newline In der zweiten Situation hat die Hyperplane eine
quadratische Form: \begin{align*}
\beta_0+\beta_1 X_1 + \beta_2 X_1^2+\beta_3 X_2+\beta_4 X_2^2+...+\beta_{2p-2}X_{p-1}+\beta_{2p-1}X_{p-1}^2=\beta_{2p} X_p
\end{align*} diese Form der Trennung stellt also eine
Merkmalserweiterung um quadratische Terme dar und funktioniert damit
ähnlich wie eine SVM mit polynomialen Kernel mit \(d=2\).Die
Verschiebung erflogte hier nur durch skalierung der Werte von \(X_p\)
ebenfalls mit \(k\sim\mathcal{N}(\mu_k,\sigma^2_k)\) für die eine
Ausprägung und \(k\sim\mathcal{N}(-\mu_k,\sigma^2)\) für die andere
Ausprägung. Der letzte DGP geht von einer noch Komplexeren
Entscheidungsgrenzen aus. Es wird hier ein Hypershpäre im \(p\)
dimensionalen Raum erstellt und einmal innerhalb und einmal außerhalb
dieser gesampelt. Dafür wurde für eine Beobachtung \(j\), \(p-1\) Winkel
\(\theta\) zufällig erstellt, ein Radius \(r\) festgelegt und
anschließend die einzelnen Werte \(X_{1,j},X_{2,j},...,X_{p,j}\)
berechnet. Die berechnung erfolgt dabei über die Definition von
sphärischen Koordinaten: \begin{align*}
        X_{1,j} &= r \cos(\theta_1)\\
        X_{2,j} &= r \sin(\theta_1)\cos(\theta_2)\\
        X_{3,j} &= r \sin(\theta_1)\sin(\theta_2)\cos(\theta_3)\\
        &\quad \vdots\\
        X_{p-1,j}&=r \sin(\theta_1)\ldots \sin(\theta_{p-2})\cos(\theta_{p-1})\\
        X_{p,j}&=r \sin(\theta_1)\ldots \sin(\theta_{p-2})\sin(\theta_{p-1})
    \end{align*} Dieser Vorgang wird dann \(n/2\) mal, wiederholt und
anschließend erfolgt die Verschiebung durch eine Skalierung des
jeweiligen normalisierten Datenvektors
\(k\left(\frac{\overline{x}}{||x||}\right)\). \(k\) ist auch hier wieder
eine Zufallsvariable mit \(\mu_k\) für die eine Ausprägung der
Zielvariable und \(-\mu_k\) für die andere Ausprägung. Die Streuung
bleibt bei beiden bei einem konstanten Werte \(\sigma^2_k\). Je nachdem
wie \(\mu_k\) und \(\sigma^2_r\) gewählt werden kann die Trennbarkeit
der Daten angepasst werden.

Es ergeben sich daher 9 unterschiedliche Datensituationen, welche in
ihren Dimensionen und Komplexität der Entscheidungsgrenze variieren. Die
Kürzel für die Situationen sind in Tabelle \ref{tab:datensituationen}
abgetragen

\begin{table}[H]
\begin{center}
\begin{tabular}{ |c|c|c|c| }
 \hline
  & linear & polynomial & radial \\
 \hline
 $p \ll n$ & S1 & S2 & S3 \\
 \hline
 $p \approx n$ & S4 & S5 & S6 \\
 \hline
 $p \gg n$ & S7 & S8 & S9 \\
 \hline
\end{tabular}
\end{center}
\caption{Datenszenarien}
\label{tab:datensituationen}
\end{table}

Zusätzlich soll nicht nur ein Vergleich zwischen der Performance der
SVMS mit verschiedenen Kernel gemacht werden, sondern auch die
Unterschiede in der Klassifikationsgüte der SVMs zu anderen gängigen
Klassifikationsmethoden gezeigt werden. Dafür werden Regularized
Logistic Regression und k-nearest Neighbour als Vergleichsalgorithmen
hinzugezogen.

\printbibliography

\end{document}
