@book{PredictiveAnalysis,
  title = {R: {{Predictive Analysis}}},
  shorttitle = {R},
  url = {https://learning.oreilly.com/library/view/r-predictive-analysis/9781788290371/},
  urldate = {2024-06-18},
  abstract = {Master the art of predictive modeling About This Book Load, wrangle, and analyze your data using the world's most powerful statistical programming language Familiarize yourself with the most...},
  isbn = {978-1-78829-037-1},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\K57NVNBJ\9781788290371.html}
}

@article{cortesSupportvectorNetworks1995,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  date = {1995-09},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {20},
  number = {3},
  pages = {273--297},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00994018},
  url = {http://link.springer.com/10.1007/BF00994018},
  urldate = {2024-06-16},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\F8S29KAF\Cortes und Vapnik - 1995 - Support-vector networks.pdf}
}

@book{jamesIntroductionStatisticalLearning2021,
  title = {An {{Introduction}} to {{Statistical Learning}}: With {{Applications}} in {{R}}},
  shorttitle = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2021},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer US},
  location = {New York, NY},
  doi = {10.1007/978-1-0716-1418-1},
  url = {https://link.springer.com/10.1007/978-1-0716-1418-1},
  urldate = {2024-06-15},
  isbn = {978-1-07-161417-4 978-1-07-161418-1},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\EKXTLWT8\James et al. - 2021 - An Introduction to Statistical Learning with Appl.pdf}
}

@inproceedings{boserTrainingAlgorithmOptimal1992,
  title = {A Training Algorithm for Optimal Margin Classifiers},
  booktitle = {Proceedings of the Fifth Annual Workshop on {{Computational}} Learning Theory},
  author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  date = {1992-07},
  pages = {144--152},
  publisher = {ACM},
  location = {Pittsburgh Pennsylvania USA},
  doi = {10.1145/130385.130401},
  url = {https://dl.acm.org/doi/10.1145/130385.130401},
  urldate = {2024-06-26},
  eventtitle = {{{COLT92}}: 5th {{Annual Workshop}} on {{Computational Learning Theory}}},
  isbn = {978-0-89791-497-0},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\BP6B8M22\Boser et al. - 1992 - A training algorithm for optimal margin classifier.pdf}
}

@incollection{kecmanSupportVectorMachines2005,
  title = {Support {{Vector Machines}} – {{An Introduction}}},
  booktitle = {Support {{Vector Machines}}: {{Theory}} and {{Applications}}},
  author = {Kecman, V.},
  editor = {Wang, Lipo},
  editora = {Kacprzyk, Janusz},
  editoratype = {redactor},
  date = {2005-04-22},
  volume = {177},
  pages = {1--47},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/10984697_1},
  url = {http://link.springer.com/10.1007/10984697_1},
  urldate = {2024-07-01},
  isbn = {978-3-540-24388-5 978-3-540-32384-6},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\DG38YE8G\Kecman - 2005 - Support Vector Machines – An Introduction.pdf}
}

@article{scholzComparisonClassificationMethods2021,
  title = {A Comparison of Classification Methods across Different Data Complexity Scenarios and Datasets},
  author = {Scholz, Michael and Wimmer, Tristan},
  date = {2021-04-15},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {168},
  pages = {114217},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2020.114217},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417420309428},
  urldate = {2024-07-01},
  abstract = {Recent research assessed the performance of classification methods mainly on concrete datasets whose statistical characteristics are unknown or unreported. The performance furthermore is often determined by only one performance measure, such as the area under the receiver operating characteristic curve. The performance of several classification methods in four different complexity scenarios and on datasets described by five data characteristics is compared in this paper. Synthetical datasets are used to control their statistical characteristics and real datasets are used to verify our findings. The performance of each classification method is determined by six measures. The investigation reveals that heterogeneous classifiers perform best on average, bagged CART is especially recommendable for datasets with low dimensionality and high sample size, kernel-based classification methods perform very well especially with a polynomial kernel, but require a rather long time for training and a nearest shrunken neighbor classifier is recommendable in case of unbalanced datasets. These findings help researchers and practitioners finding an appropriate method for their binary classification problems.},
  keywords = {Binary classification,Classification methods,Data characteristics,Performance comparison},
  file = {C\:\\Users\\49152\\Zotero\\storage\\9HE72H3B\\Scholz und Wimmer - 2021 - A comparison of classification methods across diff.pdf;C\:\\Users\\49152\\Zotero\\storage\\4TFG2L3X\\S0957417420309428.html}
}

@article{sokolovaSystematicAnalysisPerformance2009,
  title = {A Systematic Analysis of Performance Measures for Classification Tasks},
  author = {Sokolova, Marina and Lapalme, Guy},
  date = {2009-07},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {45},
  number = {4},
  pages = {427--437},
  issn = {03064573},
  doi = {10.1016/j.ipm.2009.03.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457309000259},
  urldate = {2024-08-13},
  abstract = {This paper presents a systematic analysis of twenty four performance measures used in the complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class, multi-labelled, and hierarchical. For each classification task, the study relates a set of changes in a confusion matrix to specific characteristics of data. Then the analysis concentrates on the type of changes to a confusion matrix that do not change a measure, therefore, preserve a classifier’s evaluation (measure invariance). The result is the measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem. This formal analysis is supported by examples of applications where invariance properties of measures lead to a more reliable evaluation of classifiers. Text classification supplements the discussion with several case studies.},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\R8LZLIHE\Sokolova und Lapalme - 2009 - A systematic analysis of performance measures for .pdf}
}

@book{kuhnAppliedPredictiveModeling2013,
  title = {Applied {{Predictive Modeling}}},
  author = {Kuhn, Max and Johnson, Kjell},
  date = {2013},
  publisher = {Springer New York},
  location = {New York, NY},
  doi = {10.1007/978-1-4614-6849-3},
  url = {http://link.springer.com/10.1007/978-1-4614-6849-3},
  urldate = {2024-08-13},
  isbn = {978-1-4614-6848-6 978-1-4614-6849-3},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\7FY5ALB8\Kuhn und Johnson - 2013 - Applied Predictive Modeling.pdf}
}

@article{entezari-malekiComparisonClassificationMethods2009,
  title = {Comparison of {{Classification Methods Based}} on the {{Type}} of {{Attributes}} and {{Sample Size}}},
  author = {Entezari-Maleki, Reza and Rezaei, Arash and Minaei-Bidgoli, Behrouz},
  date = {2009-09-30},
  journaltitle = {Journal of Convergence Information Technology},
  shortjournal = {JCIT},
  volume = {4},
  number = {3},
  pages = {94--102},
  issn = {1975-9320, 2233-9299},
  doi = {10.4156/jcit.vol4.issue3.14},
  url = {http://www.aicit.org/jcit/paper_detail.html?q=66},
  urldate = {2024-08-14},
  abstract = {In this paper, the efficacy of seven data classification methods; Decision Tree (DT), k-Nearest Neighbor (k-NN), Logistic Regression (LogR), Naïve Bayes (NB), C4.5, Support Vector Machine (SVM) and Linear Classifier (LC) with regard to the Area Under Curve (AUC) metric have been compared. The effects of parameters including size of the dataset, kind of the independent attributes, and the number of the discrete and continuous attributes have been investigated.},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\2RXH8VM6\- et al. - 2009 - Comparison of Classification Methods Based on the .pdf}
}

@article{faveroClassificationPerformanceEvaluation2022,
  title = {Classification {{Performance Evaluation}} from {{Multilevel Logistic}} and {{Support Vector Machine Algorithms}} through {{Simulated Data}} in {{Python}}},
  author = {Fávero, Luiz Paulo and Belfiore, Patrícia and Santos, Helder Prado and family=Santos, given=Marcos, prefix=dos, useprefix=true and family=Araújo Costa, given=Igor Pinheiro, prefix=de, useprefix=true and Junior, Wilson Tarantin},
  date = {2022-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  series = {9th {{International Conference}} on {{Information Technology}} and {{Quantitative Management}}},
  volume = {214},
  pages = {511--519},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2022.11.206},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050922019160},
  urldate = {2024-08-14},
  abstract = {This paper analyzes the performance of multilevel logistic and support vector machine algorithms when the objective is the stratification of the sample into two groups for binary classification. Under the data simulation in Python, we show that multilevel logistic models cannot correctly classify observations under certain non-linear conditions, even when defined contextual hierarchical groups and support vector classifiers generate better predictions. Python codes are provided for replication purposes.},
  keywords = {Logistic Models,Multilevel Models,Python,Simulation,Support Vector Machine},
  file = {C:\Users\49152\Zotero\storage\2JJSPA7G\S1877050922019160.html}
}

@book{vapnikEstimationDependencesBased2006,
  title = {Estimation of {{Dependences Based}} on {{Empirical Data}}},
  author = {Vapnik, Vladimir},
  date = {2006},
  series = {Information {{Science}} and {{Statistics}}},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/0-387-34239-7},
  url = {http://link.springer.com/10.1007/0-387-34239-7},
  urldate = {2024-08-14},
  isbn = {978-0-387-30865-4 978-0-387-34239-9},
  langid = {english},
  keywords = {development,philosophy,Schätzung,statistics,technology,time},
  file = {C:\Users\49152\Zotero\storage\MYF37BE4\Vapnik - 2006 - Estimation of Dependences Based on Empirical Data.pdf}
}

@article{bennettSupportVectorMachines2000,
  title = {Support {{Vector Machines}}: {{Hype}} or {{Hallelujah}}?},
  shorttitle = {Support {{Vector Machines}}},
  author = {Bennett, Kristin and Campbell, Colin},
  date = {2000-12-01},
  journaltitle = {ACM SIGKDD Explorations Newsletter},
  shortjournal = {ACM SIGKDD Explorations Newsletter},
  volume = {2},
  pages = {1--13},
  doi = {10.1145/380995.380999},
  abstract = {Support Vector Machines (SVMs) and related kernel methods have become increasingly popular tools for data mining tasks such as classification, regression, and novelty detection. The goal of this tutorial is to provide an intuitive explanation of SVMs from a geometric perspective. The classification problem is used to investigate the basic concepts behind SVMs and to examine their strengths and weaknesses from a data mining perspective. While this overview is not comprehensive, it does provide resources for those interested in further exploring SVMs.},
  file = {C:\Users\49152\Zotero\storage\CEPJELLM\Bennett und Campbell - 2000 - Support Vector Machines Hype or Hallelujah.pdf}
}

@article{burgesTutorialSupportVector1998,
  title = {A {{Tutorial}} on {{Support Vector Machines}} for {{Pattern Recognition}}},
  author = {Burges, Christopher J.C.},
  date = {1998-06-01},
  journaltitle = {Data Mining and Knowledge Discovery},
  shortjournal = {Data Mining and Knowledge Discovery},
  volume = {2},
  number = {2},
  pages = {121--167},
  issn = {1573-756X},
  doi = {10.1023/A:1009715923555},
  url = {https://doi.org/10.1023/A:1009715923555},
  urldate = {2024-08-16},
  abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
  langid = {english},
  keywords = {Artificial Intelligence,pattern recognition,statistical learning theory,support vector machines,VC dimension},
  file = {C:\Users\49152\Zotero\storage\2HYS47VE\Burges - 1998 - A Tutorial on Support Vector Machines for Pattern .pdf}
}

@incollection{BasicsSetConstrainedUnconstrained2008,
  title = {Basics of {{Set-Constrained}} and {{Unconstrained Optimization}}},
  booktitle = {An {{Introduction}} to {{Optimization}}},
  date = {2008},
  pages = {77--100},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781118033340.ch6},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118033340.ch6},
  urldate = {2024-08-18},
  abstract = {This chapter contains sections titled: Introduction Conditions for Local Minimizers},
  isbn = {978-1-118-03334-0},
  langid = {english},
  keywords = {extremizers,local minimizers,minimizer,set-constrained optimization,unconstrained optimization},
  file = {C:\Users\49152\Zotero\storage\6W2PGZQ2\9781118033340.html}
}

@book{chongIntroductionOptimization2013,
  title = {An Introduction to Optimization},
  author = {Chong, Edwin K. P. and Zak, Stanislaw H.},
  date = {2013-01-14},
  edition = {4},
  publisher = {John Wiley \& Sons Inc},
  location = {Hoboken, New Jersey},
  abstract = {Praise for the Third Edition ". . . guides and leads the reader through the learning path . . . [e]xamples are stated very clearly and the results are presented with attention to detail." —MAA Reviews   Fully updated to reflect new developments in the field, the Fourth Edition of Introduction to Optimization fills the need for accessible treatment of optimization theory and methods with an emphasis on engineering design. Basic definitions and notations are provided in addition to the related fundamental background for linear algebra, geometry, and calculus.   This new edition explores the essential topics of unconstrained optimization problems, linear programming problems, and nonlinear constrained optimization. The authors also present an optimization perspective on global search methods and include discussions on genetic algorithms, particle swarm optimization, and the simulated annealing algorithm. Featuring an elementary introduction to artificial neural networks, convex optimization, and multi-objective optimization, the Fourth Edition also offers:  A new chapter on integer programming Expanded coverage of one-dimensional methods Updated and expanded sections on linear matrix inequalities Numerous new exercises at the end of each chapter MATLAB exercises and drill problems to reinforce the discussed theory and algorithms Numerous diagrams and figures that complement the written presentation of key concepts MATLAB M-files for implementation of the discussed theory and algorithms (available via the book's website) Introduction to Optimization, Fourth Edition is an ideal textbook for courses on optimization theory and methods. In addition, the book is a useful reference for professionals in mathematics, operations research, electrical engineering, economics, statistics, and business.},
  isbn = {978-1-118-27901-4},
  langid = {Englisch},
  pagetotal = {622}
}

@online{snoekPracticalBayesianOptimization2012,
  title = {Practical {{Bayesian Optimization}} of {{Machine Learning Algorithms}}},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
  date = {2012-08-29},
  eprint = {1206.2944},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1206.2944},
  urldate = {2024-08-19},
  abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\49152\Zotero\storage\5IMLQK9S\Snoek et al. - 2012 - Practical Bayesian Optimization of Machine Learnin.pdf}
}

@article{yangHyperparameterOptimizationMachine2020,
  title = {On Hyperparameter Optimization of Machine Learning Algorithms: {{Theory}} and Practice},
  shorttitle = {On Hyperparameter Optimization of Machine Learning Algorithms},
  author = {Yang, Li and Shami, Abdallah},
  date = {2020-11-20},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {415},
  pages = {295--316},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2020.07.061},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231220311693},
  urldate = {2024-08-19},
  abstract = {Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyper-parameter configuration for machine learning models has a direct impact on the model’s performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-the-art optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively.},
  keywords = {Bayesian optimization,Genetic algorithm,Grid search,Hyper-parameter optimization,Machine learning,Particle swarm optimization},
  file = {C\:\\Users\\49152\\Zotero\\storage\\NED3ABVX\\Yang und Shami - 2020 - On hyperparameter optimization of machine learning.pdf;C\:\\Users\\49152\\Zotero\\storage\\ZNFKPCPX\\S0925231220311693.html}
}

@article{fawcettIntroductionROCAnalysis2006,
  title = {An Introduction to {{ROC}} Analysis},
  author = {Fawcett, Tom},
  date = {2006-06-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  series = {{{ROC Analysis}} in {{Pattern Recognition}}},
  volume = {27},
  number = {8},
  pages = {861--874},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2005.10.010},
  url = {https://www.sciencedirect.com/science/article/pii/S016786550500303X},
  urldate = {2024-08-20},
  abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research.},
  keywords = {Classifier evaluation,Evaluation metrics,ROC analysis},
  file = {C:\Users\49152\Zotero\storage\E86AGD86\S016786550500303X.html}
}

@article{plattProbabilisticOutputsSupport2000,
  title = {Probabilistic {{Outputs}} for {{Support Vector Machines}} and {{Comparisons}} to {{Regularized Likelihood Methods}}},
  author = {Platt, John},
  date = {2000-06-23},
  journaltitle = {Adv. Large Margin Classif.},
  shortjournal = {Adv. Large Margin Classif.},
  volume = {10},
  abstract = {The output of a classifier should be a calibrated posterior probability to enable post-processing. Standard SVMs do not provide such probabilities. One method to create probabilities is to directly train a kernel classifier with a logit link function and a regularized maximum likelihood score. However, training with a maximum likelihood score will produce non-sparse kernel machines. Instead, we train an SVM, then train the parameters of an additional sigmoid function to map the SVM outputs into probabilities. This chapter compares classification error rate and likelihood scores for an SVM plus sigmoid versus a kernel method trained with a regularized likelihood error function. These methods are tested on three data-mining-style data sets. The SVM+sigmoid yields probabilities of comparable quality to the regularized maximum likelihood kernel method, while still retaining the sparseness of the SVM.},
  file = {C:\Users\49152\Zotero\storage\5H88UYZ2\Platt - 2000 - Probabilistic Outputs for Support Vector Machines .pdf}
}
