---
title: "Pros und Cons"
author: "Franz Andersch & Niklas Münz"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    number_sections: true
    keep_tex: true
    includes:
      in_header: header.tex
    citation_package: biblatex
bibliography: Literatur.bib
---
Support Vector Machines haben ein hohes Ansehen unter den Machine Learning Algorithmen, da sie einige Vorteile mit sich bringen. Aufgrund der Idee einer Soft Margin und des "Kernel Tricks" ist die Methode sehr flexibel und kann für spezielle Anwendungsbereiche angepasst werden \parencite{bennettSupportVectorMachines2000}.Dazu sind die Ergebnisse stabil und reproduzierbar, was sie von anderen Methoden wie beispielsweise Neural Networks abhebt. Auch die Anwendung ist vergleichsweise einfach, da es eine überschaubare Anzahl an Parametern gibt (wie beispielsweise bei der \textit{SVM} mit radialem Kern nur der gamma- und cost-Parameter festzulegen ist).\newline
Durch die Möglichkeit der Nutzung verschiedener Kerne sind \textit{SVM} überaus vielseitig. Die Auswahl des Kerns ermöglicht es äußerst flexible Entscheidungsgrenzen zu formen \parencite{kuhnAppliedPredictiveModeling2013}. Dadurch können \textit{SVM} an verschiedene Datensituationen angepasst werden.\newline
Ein weiterer Vorteil ist, dass die Methode weitgehend robust gegenüber overfitting ist \parencite{kuhnAppliedPredictiveModeling2013}. Dafür verantwortlich ist der Cost-Parameter, anhand dessen der Fit an die Daten kontrolliert werden kann. Jedoch birgt dies auch Probleme (Erläuterungen im folgenden Abschnitt).\newline
Diese Vorteile resultieren in einer allgemein häufigen Nutzung von \textit{SVM} in der Wissenschaft. Sie haben folglich bewiesen, dass sie für verschiedenste Aufgaben gut funktionieren \parencite{kuhnAppliedPredictiveModeling2013}.

Trotz der vielfachen Nutzung von \textit{SVM}, bringen sie auch Nachteile mit sich. Das wohl größte Problem liegt in der Modellselektion \parencite{bennettSupportVectorMachines2000}. Wie bereits im vorherigen Abschnitt erwähnt, ist die Auswahl der Parameter von hoher Bedeutung bei der Performance und dem Fit an die Daten. So kontrollieren die Kernspezifischen Parameter und der Cost-Parameter einerseits die Komplexität und andererseits den Fit an die Daten \parencite{kuhnAppliedPredictiveModeling2013}. Dabei kann die Wahl der Parameter sowohl zu einem underfit als auch zu einem overfit führen. Jedoch haben nicht nur die Parameter einen Einfluss auf die Performance sondern bereits die Wahl des Kerns kann entscheidend sein \parencite{burgesTutorialSupportVector1998}. Je nach Datensituation können \textit{SVM} mit verschiedenen Kernen äußerst unterschiedliche Ergebnisse liefern. Dies zeigt die Sensitivität der Methode gegenüber der Wahl des Kerns und der Parameterabstimmung.\newline
Ein weiterer Nachteil ist, dass die Methode weniger intuitiv und aufwendiger anzuwenden ist als andere Algorithmen \parencite{bennettSupportVectorMachines2000}. So ist es zum Beispiel schwer Informationen aus Support Vektoren zu ziehen und es gibt keine Koeffizienten die interpretiert werden können.\newline
Zuletzt ist zu erwähnen, dass die Methode bei einer hohen Anzahl an Beobachtungen besonders rechenintensiv ist. So konnte beispielsweise gezeigt werden, dass insbesondere die \textit{SVM} mit polynomialem und radialem Kern eine hohe Rechenzeit aufweisen \parencite{scholzComparisonClassificationMethods2021}. Dabei konnten andere Methoden wie die \textit{logistische Regression} oder \textit{k-nearest Neighbour} deutlich besser abschneiden. Dies liegt daran, dass die Lösung des \textit{SVM}-Optimierungsproblems die Behebung eines quadratischen Programmierungsproblems erfordert. Da die Anzahl der zu optimierenden Parameter mit der Anzahl der Daten quadratisch zunimmt, führt dies zu einer hohen Rechenkomplexität \parencite{kecmanSupportVectorMachines2005}.
