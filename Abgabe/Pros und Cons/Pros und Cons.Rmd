---
title: "Pros und Cons"
author: "Franz Andersch & Niklas Münz"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    number_sections: true
    keep_tex: true
    includes:
      in_header: header.tex
    citation_package: biblatex
bibliography: Literatur.bib
---
Support Vector Machines haben ein hohes Ansehen unter den Machine Learning Algorithmen, da sie einige Vorteile mit sich bringen. Aufgrund der Idee eines Soft Margin und des Kernel-Tricks ist die Methode sehr flexibel und kann für spezielle Anwendungsbereiche angepasst werden \parencite{bennettSupportVectorMachines2000}. Dazu sind die Ergebnisse stabil und reproduzierbar, was sie von anderen Methoden wie beispielsweise neuronale Netze abhebt. Auch die Anwendung ist vergleichsweise einfach, da es eine überschaubare Anzahl an Parametern gibt (wie beispielsweise bei der SVM mit radialem Kern nur der gamma- und cost-Parameter festzulegen ist).\newline
Die Möglichkeiten der Nutzung verschiedener Kernel sind \textit{SVM} überaus vielseitig. Die Auswahl des Kerns ermöglicht es, äußerst flexible Entscheidungsgrenzen zu formen \parencite{kuhnAppliedPredictiveModeling2013}. Dadurch können SVM an verschiedene Datensituationen angepasst werden.\newline
Ein weiterer Vorteil ist, dass die Methode weitgehend robust gegenüber Overfitting ist \parencite{kuhnAppliedPredictiveModeling2013}. Dafür verantwortlich ist der Cost-Parameter, anhand dessen der Fit an die Daten kontrolliert werden kann. Jedoch birgt dies auch Probleme (Erläuterungen im folgenden Abschnitt). Diese Vorteile resultieren in einer allgemein häufigen Nutzung von SVM in der Wissenschaft. Sie haben folglich bewiesen, dass sie für verschiedenste Aufgaben gut funktionieren \parencite{kuhnAppliedPredictiveModeling2013}.

Trotz der vielfachen Nutzung von \textit{SVM}, bringen sie auch Nachteile mit sich. Das wohl größte Problem liegt in der Modellselektion \parencite{bennettSupportVectorMachines2000}. Wie bereits im vorherigen Abschnitt erwähnt, ist die Auswahl der Parameter von hoher Bedeutung bei der Performance und dem Fit an die Daten. So kontrollieren die Kernel-spezifischen Parameter und der Cost-Parameter einerseits die Komplexität und andererseits den Fit an die Daten \parencite{kuhnAppliedPredictiveModeling2013}. Dabei kann die Wahl der Parameter sowohl zu einem Underfit als auch zu einem Overfit führen. Jedoch haben nicht nur die Parameter einen Einfluss auf die Performance sondern bereits die Wahl des Kernels kann entscheidend sein \parencite{burgesTutorialSupportVector1998}. Je nach Datensituation können SVM mit verschiedenen Kernels äußerst unterschiedliche Ergebnisse liefern. Dies zeigt die Sensibilität der Methode gegenüber der Wahl des Kerns und der Parameterabstimmung.\newline
Ein weiterer Nachteil ist, dass die Methode weniger intuitiv und aufwendiger anzuwenden ist als andere Algorithmen \parencite{bennettSupportVectorMachines2000}. So ist es zum Beispiel schwer, Informationen aus Support-Vektoren zu ziehen und es gibt keine Koeffizienten die interpretiert werden können.\newline
Zuletzt ist zu erwähnen, dass die Methode bei einer hohen Anzahl an Beobachtungen besonders rechenintensiv ist. So konnte beispielsweise gezeigt werden, dass insbesondere die SVM mit polynomialem und radialem Kern eine hohe Rechenzeit aufweisen \parencite{scholzComparisonClassificationMethods2021}. Dabei konnten andere Methoden wie die \textit{logistische Regression} oder \textit{k-nearest Neighbour} deutlich besser abschneiden. Dies liegt daran, dass die Lösung des SVM-Optimierungsproblems die Behebung eines quadratischen Programmierungsproblems erfordert. Da die Anzahl der zu optimierenden Parameter mit der Anzahl der Daten quadratisch zunimmt, führt dies zu einer hohen Rechenkomplexität \parencite{kecmanSupportVectorMachines2005}.
