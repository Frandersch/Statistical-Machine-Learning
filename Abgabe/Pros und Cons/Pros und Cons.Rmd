---
title: "Pros und Cons"
author: "Franz Andersch & Niklas Münz"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    number_sections: true
    keep_tex: true
    includes:
      in_header: header.tex
    citation_package: biblatex
bibliography: Literatur.bib
---
\subsection{Pros}
effective in high-dimensional spaces; versatile through kernels (Seite 347, extremely flexible decision boundaries); robustness to overfitting (Seite 347, da man den cost parameter wählen kann) \parencite{kuhnAppliedPredictiveModeling2013}

\subsection{Cons}
computation time polynomial and radial \parencite{scholzComparisonClassificationMethods2021}

computationally intensive; sensitive to parameter tuning (Seite 347, kernel function parameters and cost value control complexity and control the over-fit) \parencite{kuhnAppliedPredictiveModeling2013}

\subsection{Hypothesen}
Die Performance von Support Vector Machines wird anhand verschiedener Datenszenarien untersucht. Dabei werden Logistic Regression und k-nearest Neighbour als Vergleichsalgorithmen hinzugezogen. Genauer gesagt, wird in neun verschiedene Szenarien unterschieden, welche sich durch zwei Unterteilungen ergeben: die Form der Entscheidungsgrenze sowie das Verhältnis zwischen der Anzahl an Dimensionen (p) und der Anzahl an Beobachtungen (n).Dabei werden ausschließlich binäre Klassifikationen untersucht. Es ergibt sich folgende Aufteilung:
\begin{table}[h]
\begin{center}
\begin{tabular}{ |c|c|c|c| }
 \hline
  & linear & polynomial & radial \\
 \hline
 p $\ll$ n & S1 & S2 & S3 \\
 \hline
 p $=$ n & S4 & S5 & S6 \\
 \hline
 p $\gg$ n & S7 & S8 & S9 \\
 \hline
\end{tabular}
\end{center}
\caption{Datensituationen}
\label{tab:datensituationen}
\end{table}
Im Folgenden werden Studien hinzugezogen, um eine Einschätzung der Performance in den verschiedenen Szenarien vorzunehmen und Hypothesen abzuleiten.\newline
Es konnte gezeigt werden, dass in einem Szenario, indem erheblich mehr Beobachtungen als Dimensionen und eine lineare Entscheidungsgrenze vorliegen (S1), deutliche Unterschiede zwischen SVM, k-NN und LogR bei der Diskriminationsfähigkeit auftreten \parencite{entezari-malekiComparisonClassificationMethods2009}. k-NN und lineare SVM zeigen AUC-Werte nahe 1 auf, was für eine nahezu perfekte Differenzierung der Klassen spricht. LogR hingegen hat einen Wert knapp über 0.5, was nur etwas besser als eine Zufallsauswahl ist. Darüber hinaus ist festzustellen, dass die Unterschiede deutlicher werden, je höher die Anzahl an Beobachtungen ist.\newline
Für den Fall einer radialen Entscheidungsgrenze (S3) sind die Ergebnisse ähnlich. So erreicht in diesem Beispiel eine SVM mit radialem Kernel im Vergleich zu einer LogR eine um 34% höhere Genauigkeit \parencite{faveroClassificationPerformanceEvaluation2022}.

Liegt ein Szenario vor, indem die Anzahl der Dimensionen erheblich größer ist, als die Anzahl der Beobachtungen, mit einer linearen Entscheidungsgrenze (S7), sind die Ergebnisse differenzierter zu betrachten. So schneidet die SVM mit polynomialem Kern am besten unter den genannten Algorithmen ab, jedoch die lineare SVM am schlechtesten (als Kriterium wurde die mittlere Performance über 100 Datensätze evaluiert) \parencite{scholzComparisonClassificationMethods2021}. Während k-NN auch in diesem Szenario eine gute Performance hat, schneiden LogR und SVM mit radialem Kern mittelmäßig ab.

Basierend auf den Ergebnissen der genannten Studien können Schlussfolgerungen gezogen werden. Es ist anzunehmen, dass in niedrigdimensionalen Szenarien k-NN und SVM´s besser performen als LogR. Jedoch ist zu vermuten, dass die Wahl des Kerns bei SVM´s einen großen Einfluss auf die Performance hat.\newline
In hochdimensionalen Szenarien zeigt vermutlich die SVM mit polynomialem oder radialem Kern eine gute Performance, unabhängig von der Form der Entscheidungsgrenze, während die lineare SVM voraussichtlich weniger gut abschneiden wird. Es scheint so, dass auch k-NN und LogR in hochdimensionalen Szenarien zumindest mittelmäßig abschneiden. Hier ist jedoch zu beachten, dass nur eine lineare Entscheidungsgrenze betrachtet wurde und in den Szenarien S8 und S9 andere Ergebnisse möglich sind.\newline
