% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage[german]{babel}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{csquotes}
\AtBeginDocument{
\renewcommand{\maketitle}{}
}
\PassOptionsToPackage{a4paper,margin = 2.5cm}{geometry}
\usepackage{geometry}
\newcommand{\bcenter}{\begin{center}}
\newcommand{\ecenter}{\end{center}}
\renewcommand{\contentsname}{Inhalt}
\usepackage{blindtext}
\usepackage[backend=biber, style = apa]{biblatex}
\addbibresource{Literatur.bib}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{biblatex}
\addbibresource{Literatur.bib}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Pros und Cons},
  pdfauthor={Franz Andersch \& Niklas Münz},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Pros und Cons}
\author{Franz Andersch \& Niklas Münz}
\date{2024-08-19}

\begin{document}
\maketitle

\subsection{Pros}

Support Vector Machines haben ein hohes Ansehen unter den Machine
Learning Algorithmen, da sie einige Vorteile mit sich bringen. Aufgrund
der Idee einer Soft Margin und des ``Kernel Tricks'' ist die Methode
sehr flexibel und kann für spezielle Anwendungsbereiche angepasst werden
\parencite{bennettSupportVectorMachines2000}.Dazu sind die Ergebnisse
stabil und reproduzierbar, was sie von anderen Methoden wie
beispielsweise Neural Networks abhebt. Auch die Anwendung ist
vergleichsweise einfach, da es eine überschaubare Anzahl an Parametern
gibt (wie beispielsweise bei der SVM mit radialem Kern nur der gamma-
und cost-Parameter festzulegen ist).\newline Durch die Möglichkeit der
Nutzung verschiedener Kerne sind SVM´s überaus vielseitig. Die Auswahl
des Kerns ermöglicht es äußerst flexible Entscheidungsgrenzen zu formen
\parencite{kuhnAppliedPredictiveModeling2013}. Dadurch können SVM´s an
verschiedene Datensituationen angepasst werden.\newline Ein weiterer
Vorteil ist, dass die Methode weitgehend robust gegenüber overfitting
ist \parencite{kuhnAppliedPredictiveModeling2013}. Dafür verantwortlich
ist der Cost-Parameter, anhand dessen der Fit an die Daten kontrolliert
werden kann. Jedoch birgt dies auch Probleme (Erläuterungen im folgenden
Abschnitt).\newline Diese Vorteile resultieren in einer allgemein
häufigen Nutzung von SVM´s in der Wissenschaft. Sie haben folglich
bewiesen, dass sie für verschiedenste Aufgaben gut funktionieren
\parencite{kuhnAppliedPredictiveModeling2013}.

\subsection{Cons}

Trotz der vielfachen Nutzung von SVM´s, bringen sie auch Nachteile mit
sich. Das wohl größte Problem liegt in der Modellselektion
\parencite{bennettSupportVectorMachines2000}. Wie bereits im vorherigen
Abschnitt erwähnt, ist die Auswahl der Parameter von hoher Bedeutung bei
der Performance und dem Fit an die Daten. So kontrollieren die
Kernspezifischen Parameter und der Cost-Parameter einerseits die
Komplexität und andererseits den Fit an die Daten
\parencite{kuhnAppliedPredictiveModeling2013}. Dabei kann die Wahl der
Parameter sowohl zu einem underfit als auch zu einem overfit führen.
Jedoch haben nicht nur die Parameter einen Einfluss auf die Performance
sondern bereits die Wahl des Kerns kann entscheidend sein
\parencite{burgesTutorialSupportVector1998}. Je nach Datensituation
können SVM´s mit verschiedenen Kernen äußert unterschiedliche Ergebnisse
liefern. Dies zeigt die Sensitivität der Methode gegenüber der Wahl des
Kerns und der Parameterabstimmung.\newline Ein weiterer Nachteil ist,
dass die Methode weniger intuitiv und aufwendiger anzuwenden ist als
andere Algorithmen \parencite{bennettSupportVectorMachines2000}. So ist
es zum Beispiel schwer Informationen aus Support Vektoren zu ziehen und
es gibt keine Koeffizienten die interpretiert werden können.\newline
Zuletzt ist zu erwähnen, dass die Methode bei einer hohen Anzahl an
Beobachtungen besonders rechenintensiv ist. So konnte beispielsweise
gezeigt werden, dass insbesondere die SVM mit polynomialem und radialem
Kern eine hohe Rechenzeit aufweisen
\parencite{scholzComparisonClassificationMethods2021}. Dabei konnten
andere Methoden wie die logistische Regression oder k-nearest Neighbour
deutlich besser abschneiden. Dies liegt daran, dass die Lösung des
SVM-Optimierungsproblems die Behebung eines quadratischen
Programmierungsproblems erfordert. Da die Anzahl der zu optimierenden
Parameter mit der Anzahl der Daten quadratisch zunimmt, führt dies zu
einer hohen Rechenkomplexität
\parencite{kecmanSupportVectorMachines2005}.

\subsection{Hypothesen}

Die Performance von Support Vector Machines wird anhand verschiedener
Datenszenarien untersucht. Dabei werden Logistic Regression und
k-nearest Neighbour als Vergleichsalgorithmen hinzugezogen. Genauer
gesagt, wird in neun verschiedene Szenarien unterschieden, welche sich
durch zwei Unterteilungen ergeben: die Form der Entscheidungsgrenze
sowie das Verhältnis zwischen der Anzahl an Dimensionen (p) und der
Anzahl an Beobachtungen (n).Dabei werden ausschließlich binäre
Klassifikationen untersucht. Es ergibt sich folgende Aufteilung in
Tabelle 1.

\begin{table}[h]
\begin{center}
\begin{tabular}{ |c|c|c|c| }
 \hline
  & linear & polynomial & radial \\
 \hline
 p $\ll$ n & S1 & S2 & S3 \\
 \hline
 p $=$ n & S4 & S5 & S6 \\
 \hline
 p $\gg$ n & S7 & S8 & S9 \\
 \hline
\end{tabular}
\end{center}
\caption{Datensituationen}
\label{tab:datensituationen}
\end{table}

Im Folgenden werden Studien hinzugezogen, um eine Einschätzung der
Performance in den verschiedenen Szenarien vorzunehmen und Hypothesen
abzuleiten. Vorab ist zu erwähnen, dass die Evaluation von
Klassifikationsmethoden anhand synthetischer Datensätze in der Literatur
begrenzt ist. Da für diese Arbeit die Form der Entscheidungsgrenze
entscheidend ist, werden dennoch auschließlich Arbeiten mit
synthetischen Datensätzen zu Rate gezogen.\newline Es konnte gezeigt
werden, dass in einem Szenario, indem erheblich mehr Beobachtungen als
Dimensionen und eine lineare Entscheidungsgrenze vorliegen (S1),
deutliche Unterschiede zwischen SVM, k-NN und logistischer Regression
bei der Diskriminationsfähigkeit auftreten
\parencite{entezari-malekiComparisonClassificationMethods2009}. k-NN und
lineare SVM zeigen AUC-Werte nahe 1 auf, was für eine nahezu perfekte
Differenzierung der Klassen spricht. Die logistische Regression hingegen
hat einen Wert knapp über 0.5, was nur etwas besser als eine
Zufallsauswahl ist. Darüber hinaus ist festzustellen, dass die
Unterschiede deutlicher werden, je höher die Anzahl an Beobachtungen
ist.\newline Für den Fall einer radialen Entscheidungsgrenze (S3) sind
die Ergebnisse ähnlich. So erreicht in diesem Beispiel eine SVM mit
radialem Kernel im Vergleich zu einer logistischen Regression eine um
34\% höhere Genauigkeit
\parencite{faveroClassificationPerformanceEvaluation2022}.

Die Szenarien S4 bis S6 finden in der Literatur kaum Beachtung, weshalb
hier keine Studien herangezogen werden können. Liegt ein Szenario vor,
indem die Anzahl der Dimensionen erheblich größer ist, als die Anzahl
der Beobachtungen, mit einer linearen Entscheidungsgrenze (S7), sind die
Ergebnisse differenzierter zu betrachten. So schneidet die SVM mit
polynomialem Kern am besten unter den genannten Algorithmen ab, jedoch
die lineare SVM am schlechtesten (als Kriterium wurde die mittlere
Performance über 100 Datensätze evaluiert)
\parencite{scholzComparisonClassificationMethods2021}. Während k-NN auch
in diesem Szenario eine gute Performance hat, schneiden logistische
Regression und SVM mit radialem Kern mittelmäßig ab. Hierbei ist wichtig
zu erwähnen, dass in der Studie keine Ergebnisse über die genaue
Performance präsentiert wurden, sondern lediglich die Ranks der 25
behandelten Klassifikationsmethoden. Somit können nur eingeschränkte
Schlussfolgerungen gezogen werden.

Basierend auf den Ergebnissen der genannten Studien können
Schlussfolgerungen gezogen werden. Es ist anzunehmen, dass in
niedrigdimensionalen Szenarien k-NN und SVM´s besser performen als eine
logistische Regression. Jedoch ist zu vermuten, dass die Wahl des Kerns
bei SVM´s einen großen Einfluss auf die Performance hat.\newline In
hochdimensionalen Szenarien zeigt vermutlich die SVM mit polynomialem
oder radialem Kern eine gute Performance, unabhängig von der Form der
Entscheidungsgrenze, während die lineare SVM voraussichtlich weniger gut
abschneiden wird. Es scheint so, dass auch k-NN und logistische
Regression in hochdimensionalen Szenarien zumindest mittelmäßig
abschneiden. Hier ist jedoch zu beachten, dass nur eine lineare
Entscheidungsgrenze betrachtet wurde und in den Szenarien S8 und S9
andere Ergebnisse möglich sind.

\printbibliography

\end{document}
