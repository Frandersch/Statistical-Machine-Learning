@book{PredictiveAnalysis,
  title = {R: {{Predictive Analysis}}},
  shorttitle = {R},
  url = {https://learning.oreilly.com/library/view/r-predictive-analysis/9781788290371/},
  urldate = {2024-06-18},
  abstract = {Master the art of predictive modeling About This Book Load, wrangle, and analyze your data using the world's most powerful statistical programming language Familiarize yourself with the most...},
  isbn = {978-1-78829-037-1},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\K57NVNBJ\9781788290371.html}
}

@article{cortesSupportvectorNetworks1995,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  date = {1995-09},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {20},
  number = {3},
  pages = {273--297},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00994018},
  url = {http://link.springer.com/10.1007/BF00994018},
  urldate = {2024-06-16},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\F8S29KAF\Cortes und Vapnik - 1995 - Support-vector networks.pdf}
}

@book{jamesIntroductionStatisticalLearning2021,
  title = {An {{Introduction}} to {{Statistical Learning}}: With {{Applications}} in {{R}}},
  shorttitle = {An {{Introduction}} to {{Statistical Learning}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2021},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer US},
  location = {New York, NY},
  doi = {10.1007/978-1-0716-1418-1},
  url = {https://link.springer.com/10.1007/978-1-0716-1418-1},
  urldate = {2024-06-15},
  isbn = {978-1-07-161417-4 978-1-07-161418-1},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\EKXTLWT8\James et al. - 2021 - An Introduction to Statistical Learning with Appl.pdf}
}

@inproceedings{boserTrainingAlgorithmOptimal1992,
  title = {A Training Algorithm for Optimal Margin Classifiers},
  booktitle = {Proceedings of the Fifth Annual Workshop on {{Computational}} Learning Theory},
  author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  date = {1992-07},
  pages = {144--152},
  publisher = {ACM},
  location = {Pittsburgh Pennsylvania USA},
  doi = {10.1145/130385.130401},
  url = {https://dl.acm.org/doi/10.1145/130385.130401},
  urldate = {2024-06-26},
  eventtitle = {{{COLT92}}: 5th {{Annual Workshop}} on {{Computational Learning Theory}}},
  isbn = {978-0-89791-497-0},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\BP6B8M22\Boser et al. - 1992 - A training algorithm for optimal margin classifier.pdf}
}

@incollection{kecmanSupportVectorMachines2005,
  title = {Support {{Vector Machines}} – {{An Introduction}}},
  booktitle = {Support {{Vector Machines}}: {{Theory}} and {{Applications}}},
  author = {Kecman, V.},
  editor = {Wang, Lipo},
  editora = {Kacprzyk, Janusz},
  editoratype = {redactor},
  date = {2005-04-22},
  volume = {177},
  pages = {1--47},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/10984697_1},
  url = {http://link.springer.com/10.1007/10984697_1},
  urldate = {2024-07-01},
  isbn = {978-3-540-24388-5 978-3-540-32384-6},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\DG38YE8G\Kecman - 2005 - Support Vector Machines – An Introduction.pdf}
}

@article{scholzComparisonClassificationMethods2021,
  title = {A Comparison of Classification Methods across Different Data Complexity Scenarios and Datasets},
  author = {Scholz, Michael and Wimmer, Tristan},
  date = {2021-04-15},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {168},
  pages = {114217},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2020.114217},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417420309428},
  urldate = {2024-07-01},
  abstract = {Recent research assessed the performance of classification methods mainly on concrete datasets whose statistical characteristics are unknown or unreported. The performance furthermore is often determined by only one performance measure, such as the area under the receiver operating characteristic curve. The performance of several classification methods in four different complexity scenarios and on datasets described by five data characteristics is compared in this paper. Synthetical datasets are used to control their statistical characteristics and real datasets are used to verify our findings. The performance of each classification method is determined by six measures. The investigation reveals that heterogeneous classifiers perform best on average, bagged CART is especially recommendable for datasets with low dimensionality and high sample size, kernel-based classification methods perform very well especially with a polynomial kernel, but require a rather long time for training and a nearest shrunken neighbor classifier is recommendable in case of unbalanced datasets. These findings help researchers and practitioners finding an appropriate method for their binary classification problems.},
  keywords = {Binary classification,Classification methods,Data characteristics,Performance comparison},
  file = {C\:\\Users\\49152\\Zotero\\storage\\9HE72H3B\\Scholz und Wimmer - 2021 - A comparison of classification methods across diff.pdf;C\:\\Users\\49152\\Zotero\\storage\\4TFG2L3X\\S0957417420309428.html}
}

@article{sokolovaSystematicAnalysisPerformance2009,
  title = {A Systematic Analysis of Performance Measures for Classification Tasks},
  author = {Sokolova, Marina and Lapalme, Guy},
  date = {2009-07},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {45},
  number = {4},
  pages = {427--437},
  issn = {03064573},
  doi = {10.1016/j.ipm.2009.03.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457309000259},
  urldate = {2024-08-13},
  abstract = {This paper presents a systematic analysis of twenty four performance measures used in the complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class, multi-labelled, and hierarchical. For each classification task, the study relates a set of changes in a confusion matrix to specific characteristics of data. Then the analysis concentrates on the type of changes to a confusion matrix that do not change a measure, therefore, preserve a classifier’s evaluation (measure invariance). The result is the measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem. This formal analysis is supported by examples of applications where invariance properties of measures lead to a more reliable evaluation of classifiers. Text classification supplements the discussion with several case studies.},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\R8LZLIHE\Sokolova und Lapalme - 2009 - A systematic analysis of performance measures for .pdf}
}

@book{kuhnAppliedPredictiveModeling2013,
  title = {Applied {{Predictive Modeling}}},
  author = {Kuhn, Max and Johnson, Kjell},
  date = {2013},
  publisher = {Springer New York},
  location = {New York, NY},
  doi = {10.1007/978-1-4614-6849-3},
  url = {http://link.springer.com/10.1007/978-1-4614-6849-3},
  urldate = {2024-08-13},
  isbn = {978-1-4614-6848-6 978-1-4614-6849-3},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\7FY5ALB8\Kuhn und Johnson - 2013 - Applied Predictive Modeling.pdf}
}

@article{-ComparisonClassificationMethods2009,
  title = {Comparison of {{Classification Methods Based}} on the {{Type}} of {{Attributes}} and {{Sample Size}}},
  author = {-, Reza EntezariMaleki and -, Arash Rezaei and -, Behrouz MinaeiBidgoli},
  date = {2009-09-30},
  journaltitle = {Journal of Convergence Information Technology},
  shortjournal = {JCIT},
  volume = {4},
  number = {3},
  pages = {94--102},
  issn = {1975-9320, 2233-9299},
  doi = {10.4156/jcit.vol4.issue3.14},
  url = {http://www.aicit.org/jcit/paper_detail.html?q=66},
  urldate = {2024-08-14},
  abstract = {In this paper, the efficacy of seven data classification methods; Decision Tree (DT), k-Nearest Neighbor (k-NN), Logistic Regression (LogR), Naïve Bayes (NB), C4.5, Support Vector Machine (SVM) and Linear Classifier (LC) with regard to the Area Under Curve (AUC) metric have been compared. The effects of parameters including size of the dataset, kind of the independent attributes, and the number of the discrete and continuous attributes have been investigated.},
  langid = {english},
  file = {C:\Users\49152\Zotero\storage\2RXH8VM6\- et al. - 2009 - Comparison of Classification Methods Based on the .pdf}
}

@article{faveroClassificationPerformanceEvaluation2022,
  title = {Classification {{Performance Evaluation}} from {{Multilevel Logistic}} and {{Support Vector Machine Algorithms}} through {{Simulated Data}} in {{Python}}},
  author = {Fávero, Luiz Paulo and Belfiore, Patrícia and Santos, Helder Prado and family=Santos, given=Marcos, prefix=dos, useprefix=true and family=Araújo Costa, given=Igor Pinheiro, prefix=de, useprefix=true and Junior, Wilson Tarantin},
  date = {2022-01-01},
  journaltitle = {Procedia Computer Science},
  shortjournal = {Procedia Computer Science},
  series = {9th {{International Conference}} on {{Information Technology}} and {{Quantitative Management}}},
  volume = {214},
  pages = {511--519},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2022.11.206},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050922019160},
  urldate = {2024-08-14},
  abstract = {This paper analyzes the performance of multilevel logistic and support vector machine algorithms when the objective is the stratification of the sample into two groups for binary classification. Under the data simulation in Python, we show that multilevel logistic models cannot correctly classify observations under certain non-linear conditions, even when defined contextual hierarchical groups and support vector classifiers generate better predictions. Python codes are provided for replication purposes.},
  keywords = {Logistic Models,Multilevel Models,Python,Simulation,Support Vector Machine},
  file = {C:\Users\49152\Zotero\storage\2JJSPA7G\S1877050922019160.html}
}

@book{vapnikEstimationDependencesBased2006,
  title = {Estimation of {{Dependences Based}} on {{Empirical Data}}},
  author = {Vapnik, Vladimir},
  date = {2006},
  series = {Information {{Science}} and {{Statistics}}},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/0-387-34239-7},
  url = {http://link.springer.com/10.1007/0-387-34239-7},
  urldate = {2024-08-14},
  isbn = {978-0-387-30865-4 978-0-387-34239-9},
  langid = {english},
  keywords = {development,philosophy,Schätzung,statistics,technology,time},
  file = {C:\Users\49152\Zotero\storage\MYF37BE4\Vapnik - 2006 - Estimation of Dependences Based on Empirical Data.pdf}
}
